{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ade21e3",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d82341",
   "metadata": {},
   "source": [
    "## 1.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf111dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard dependencies\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf152f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow dependencies - Functional API\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "484caf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow dependencies - Functional API\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten, ZeroPadding2D, Convolution2D, Dropout, Activation\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04db90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Preprocessing packages\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "052b9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2064e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd1c9e0",
   "metadata": {},
   "source": [
    "## 1.3 Set GPU Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1efc582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aad3c364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61da485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable first GPU\n",
    "  tf.config.set_visible_devices(physical_devices[1], 'GPU')\n",
    "  logical_devices = tf.config.list_logical_devices('GPU')\n",
    "  # Logical device was not created for first GPU\n",
    "  assert len(logical_devices) == len(physical_devices) - 1\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6010b3ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "TensorFlow device (GPU:0) is being mapped to multiple devices (0 now, and 1 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not currently supported, see https://github.com/tensorflow/tensorflow/issues/19083",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8b7580fab5c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_local_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/test/jupyterenv/lib/python3.6/site-packages/tensorflow/python/client/device_lib.py\u001b[0m in \u001b[0;36mlist_local_devices\u001b[0;34m(session_config)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mserialized_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   return [\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pywrap_device_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m   ]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: TensorFlow device (GPU:0) is being mapped to multiple devices (0 now, and 1 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not currently supported, see https://github.com/tensorflow/tensorflow/issues/19083"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588442d4",
   "metadata": {},
   "source": [
    "# 2. Load and Preprocess Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fed8f8",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing - Scale and Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path, required_size=(224,224)):\n",
    "    raw = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(raw, channels=3, dct_method='INTEGER_ACCURATE')\n",
    "    image = tf.image.resize(image,required_size, method='nearest')\n",
    "    image = tf.cast(image, 'float32')\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04e7626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_preprocess(image_path, required_size=(224,224)): \n",
    "    # load image and detect the face\n",
    "    image = preprocess(image_path)\n",
    "    \n",
    "    #image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    #Preprocessing\n",
    "    face_array = preprocess_input(image)\n",
    "    \n",
    "    \n",
    "    # Scale image to be between 0 and 1 \n",
    "    face_array = (face_array - np.amin(face_array)) / (np.amax(face_array) - np.amin(face_array))\n",
    "    # Scale image to be between -1 and 1\n",
    "    face_array = 2*face_array - 1\n",
    "    \n",
    "    return tf.convert_to_tensor(face_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d66e73",
   "metadata": {},
   "source": [
    "# 2.2 Create Labelled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a78600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data from the csv files\n",
    "df = pd.read_csv('cross_validation_data/siamese_training_data_cv_1.csv')\n",
    "\n",
    "anchor_array = df.anchor.to_list()\n",
    "full_array = df.other_image.to_list()\n",
    "binary_array = df.binary.to_list()\n",
    "anchor_label = df.anchor_label.to_list()\n",
    "other_image_labels = df.other_image_label.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa19910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a tf dataset\n",
    "data = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(anchor_array)\n",
    "                            ,tf.data.Dataset.from_tensor_slices(full_array), \n",
    "                            tf.data.Dataset.from_tensor_slices(binary_array),\n",
    "                            tf.data.Dataset.from_tensor_slices(anchor_label),\n",
    "                            tf.data.Dataset.from_tensor_slices(other_image_labels)\n",
    "                           ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d63287",
   "metadata": {},
   "source": [
    "# 2.3 Build Train and Test Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afaf768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess function to map file names to the preprocessed images into tensors\n",
    "def preprocess_twin(anchor_img, other_img, label, a_lable, o_label):\n",
    "    return(complete_preprocess(anchor_img), complete_preprocess(other_img), label, a_lable, o_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloader pipeline\n",
    "data = data.map(lambda x, y, z, a, b: tf.py_function(preprocess_twin, inp = (x, y, z, a, b), Tout=(tf.float32, tf.float32, tf.int32, tf.int32, tf.int32)))\n",
    "#data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024, seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training partition\n",
    "train_data = data.take(round(len(data)*0.8)) \n",
    "#train_data = train_data.cache()\n",
    "train_data = train_data.batch(32)\n",
    "#train_data = train_data.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c36a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation partition\n",
    "test_data = data.skip(round(len(data)*0.8))\n",
    "test_data = test_data.take(round(len(data)*0.2))\n",
    "#test_data = train_data.cache()\n",
    "test_data = test_data.batch(32)\n",
    "#test_data = test_data.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f278b5",
   "metadata": {},
   "source": [
    "# 3. Model Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a93a1",
   "metadata": {},
   "source": [
    "## 3.1.5 Load VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "842b1942",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = keras.Sequential()\n",
    "vgg_model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "vgg_model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "vgg_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "vgg_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "vgg_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "vgg_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "vgg_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vgg_model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "vgg_model.add(Dropout(0.5))\n",
    "vgg_model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "vgg_model.add(Dropout(0.5))\n",
    "vgg_model.add(Convolution2D(2622, (1, 1)))\n",
    "vgg_model.add(Flatten())\n",
    "vgg_model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50a9ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "vgg_model.load_weights('vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a221d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_face_embedding = Model(inputs=vgg_model.layers[0].input, outputs=vgg_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1101762d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Freeze four convolution blocks\n",
    "for layer in vgg_face_embedding.layers:\n",
    "    layer.trainable = False\n",
    "# Make sure you have frozen the correct layers\n",
    "for i, layer in enumerate(vgg_face_embedding.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1c871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg_face_embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40b3c5",
   "metadata": {},
   "source": [
    "## 3.16 Facenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6394f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inception_resnet_v1 import *\n",
    "facenet_model = InceptionResNetV1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd958e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "#pre-trained weights https://drive.google.com/file/d/1971Xk5RwedbudGgTIrGAL4F7Aifu7id1/view?usp=sharing\n",
    "facenet_model.load_weights('facenet_weights.h5')\n",
    " \n",
    "facenet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c82668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Freeze four convolution blocks\n",
    "for layer in facenet_model.layers:\n",
    "    layer.trainable = False\n",
    "# Make sure you have frozen the correct layers\n",
    "for i, layer in enumerate(facenet_model.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb721a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(x):\n",
    "    return x / np.sqrt(np.sum(np.multiply(x, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90787a98",
   "metadata": {},
   "source": [
    "## 3.2 Build Distance Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8644f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese L1 Distance class (custom layer)\n",
    "class L1Dist(Layer):\n",
    "    \n",
    "    # Init method - inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "       \n",
    "    # Similarity calculation\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cf4ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese L2 Distance class (custom layer)\n",
    "class L2Dist(Layer):\n",
    "    \n",
    "    # Init method - inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "       \n",
    "    # Similarity calculation\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        sum_square = tf.math.reduce_sum(tf.math.square(input_embedding - validation_embedding), axis=1, keepdims=True)\n",
    "        return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8ec66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L2 Norm class (custom layer)\n",
    "class L2Norm(Layer):\n",
    "    \n",
    "    # Init method - inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "       \n",
    "    # Similarity calculation\n",
    "    def call(self, x):\n",
    "        return x / tf.math.sqrt(tf.math.reduce_sum(tf.math.multiply(x, x), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55493d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = L1Dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5b92cfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anchor_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-48b6f0b2098a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'anchor_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "l1(anchor_embedding, validation_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f878b3",
   "metadata": {},
   "source": [
    "## 3.3 Make Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d17f4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model(model,required_size=(224,224,3)): \n",
    "    \n",
    "    # Anchor image input in the network\n",
    "    input_image = Input(name='input_img', shape=required_size)\n",
    "    \n",
    "    # Validation image in the network \n",
    "    validation_image = Input(name='validation_img', shape=required_size)\n",
    "    \n",
    "    # Combine siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    norm1 = L2Norm()(model(input_image))\n",
    "    norm2 = L2Norm()(model(validation_image))\n",
    "    distances = siamese_layer(norm1, norm2)\n",
    "    \n",
    "    # Classification layer \n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc66311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model(vgg_face_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8becf4bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_img (InputLayer)          [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "validation_img (InputLayer)     [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 2622)         145002878   input_img[0][0]                  \n",
      "                                                                 validation_img[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "l2_norm (L2Norm)                (None, 2622)         0           model[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "l2_norm_1 (L2Norm)              (None, 2622)         0           model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "distance (L1Dist)               (None, 2622)         0           l2_norm[0][0]                    \n",
      "                                                                 l2_norm_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2623        distance[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 145,005,501\n",
      "Trainable params: 2,623\n",
      "Non-trainable params: 145,002,878\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31313f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_file_name = 'cross_validation_trained_models/siamesemodel_l1Dist_vgg_with_hard_mining_cv_1.h5'\n",
    "\n",
    "# Reload model\n",
    "siamese_model.load_weights(siamese_file_name, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19afb16e",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633bc74a",
   "metadata": {},
   "source": [
    "## 4.1 Setup Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c166447",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
    "#Set BinaryCrossentropy(from_logits=True) if the input of the loss function are not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd5582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a learning rate scheduler\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99835e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without learning rate decay\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01) # 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7098e0b",
   "metadata": {},
   "source": [
    "## 4.2 Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define functions to create the triplet loss with online triplet mining.\"\"\"\n",
    "#Code and information: https://omoindrot.github.io/triplet-loss\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product = tf.linalg.matmul(embeddings, tf.transpose(embeddings))\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    square_norm = tf.linalg.diag_part(dot_product)\n",
    "\n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot_product + tf.expand_dims(square_norm, 0)\n",
    "\n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "\n",
    "    if not squared:\n",
    "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "        # we need to add a small epsilon where distances == 0.0\n",
    "        mask = tf.dtypes.cast(tf.equal(distances, 0.0), tf.float32)\n",
    "        distances = distances + mask * 1e-16\n",
    "\n",
    "        distances = tf.sqrt(distances)\n",
    "\n",
    "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
    "        distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i and j are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "\n",
    "    # Check if labels[i] == labels[j]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "def batch_hard_binary_sampling(images,labels, embeddings, squared=False):\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    mask_anchor_positive = tf.dtypes.cast(mask_anchor_positive, tf.float32)\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_positive_dist\", tf.reduce_mean(hardest_positive_dist))\n",
    "\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = tf.dtypes.cast(mask_anchor_negative, tf.float32)\n",
    "\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "    # shape (batch_size,)\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_negative_dist\", tf.reduce_mean(hardest_negative_dist))\n",
    "    \n",
    "    \n",
    "    #Number of triplets\n",
    "    triplet_number = tf.dtypes.cast(tf.math.count_nonzero(hardest_positive_dist), tf.int32)\n",
    "    \n",
    "    #List of all the available positives (0's indivate that the picture has no positive and therefore no triple)\n",
    "    available_positives_mask = tf.logical_not(tf.math.equal(hardest_positive_dist,0.0))\n",
    "    available_positives = tf.dtypes.cast(available_positives_mask, tf.float32)\n",
    "    \n",
    "    # Find the position of all the max positive distances and min negative distances\n",
    "    positive_max_position = tf.math.argmax(anchor_positive_dist, 1)\n",
    "    negative_min_position = tf.math.argmin(anchor_negative_dist, 1)\n",
    "\n",
    "    \n",
    "    # Find all the positions where the positives are only 0, this is likely and indication there are no positives\n",
    "    # It could also be there are\n",
    "    hard_pos = hardest_positive_dist.numpy()\n",
    "    zero_positions = np.where(hard_pos == 0)[0]\n",
    "    \n",
    "    # Making sure not to remove cases where images are close to identical (distance between positivies is 0)\n",
    "    n_labels = list(labels.numpy())\n",
    "    contains_positives = [n_labels[element] for element in zero_positions if n_labels.count(n_labels[element]) > 1]\n",
    "    for c in contains_positives:\n",
    "        zero_positions = np.delete(zero_positions, np.where(c == 0))\n",
    "    \n",
    "    #Remove the anchors that have no positives\n",
    "    pos_max = np.delete(positive_max_position.numpy(), zero_positions)\n",
    "    neg_min = np.delete(negative_min_position.numpy(), zero_positions)\n",
    "    all_images = images.numpy()\n",
    "    \n",
    "    anchors = []\n",
    "    other = []\n",
    "    binary = []\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(pos_max)):\n",
    "        anchors.append(all_images[i])\n",
    "        other.append(all_images[pos_max[i]])\n",
    "        binary.append(1.0)\n",
    "        \n",
    "        anchors.append(all_images[i])\n",
    "        other.append(all_images[neg_min[i]])\n",
    "        binary.append(0.0)\n",
    "    \n",
    "    return tf.convert_to_tensor(anchors), tf.convert_to_tensor(other), tf.dtypes.cast(tf.convert_to_tensor(binary), tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baeaa22",
   "metadata": {},
   "source": [
    "## 4.3 Build Train Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe61dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the metrics\n",
    "train_acc_metric = keras.metrics.BinaryAccuracy()\n",
    "val_acc_metric = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "train_loss_mean = tf.keras.metrics.Mean()\n",
    "val_loss_mean = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch, epoch):\n",
    "    #if epoch % 10 == 0:\n",
    "    if False:\n",
    "        new = tf.concat([batch[0], batch[1]], 0)\n",
    "        embedding = facenet_model(new)\n",
    "        labels = tf.concat([batch[3], batch[4]], 0)\n",
    "        anchor, other, binary = tf.py_function(batch_hard_binary_sampling, inp=[new, labels,embedding], \n",
    "                                               Tout=[tf.float32, tf.float32, tf.float32])\n",
    "        X = list([anchor,other])\n",
    "        Y = binary\n",
    "    else:\n",
    "        X = batch[:2]\n",
    "        Y = batch[2]\n",
    "        print(batch)\n",
    "    \n",
    "    # Record all of our operations \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        yhat = siamese_model(X, training=True)\n",
    "        # Calculate loss\n",
    "        loss = binary_cross_loss(Y, yhat)\n",
    "        \n",
    "    # Calculate gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    \n",
    "    # Calculate updated weights and apply to siamese model\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "    \n",
    "    #Update loss metric\n",
    "    train_loss_mean.update_state(loss)\n",
    "    \n",
    "    #Updating training metric\n",
    "    train_acc_metric.update_state(Y, yhat)\n",
    "    \n",
    "    # Return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59381d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(batch, epoch):\n",
    "    \n",
    "    #if epoch % 10 == 0:\n",
    "    if False:\n",
    "        new = tf.concat([batch[0], batch[1]], 0)\n",
    "        embedding = facenet_model(new)\n",
    "        labels = tf.concat([batch[3], batch[4]], 0)\n",
    "        anchor, other, binary = tf.py_function(batch_hard_binary_sampling, inp=[new, labels,embedding], \n",
    "                                               Tout=[tf.float32, tf.float32, tf.float32])\n",
    "        X = list([anchor,other])\n",
    "        Y = binary\n",
    "    else:\n",
    "        X = batch[:2]\n",
    "        Y = batch[2]\n",
    "        \n",
    "    # Forward pass\n",
    "    val_logits = siamese_model(X, training=False)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = binary_cross_loss(Y, val_logits)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Updating validation metric\n",
    "    val_acc_metric.update_state(Y, val_logits)\n",
    "    val_loss_mean.update_state(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac8d898",
   "metadata": {},
   "source": [
    "## 4.4 Build Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d6110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, v_data, EPOCHS):\n",
    "    epoch_l = []\n",
    "    t_loss = []\n",
    "    t_acc = []\n",
    "    v_loss = []\n",
    "    v_acc = []\n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        epoch_l.append(epoch)\n",
    "        \n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step here\n",
    "            loss_value = train_step(batch,epoch)\n",
    "            progbar.update(idx+1)\n",
    "        \n",
    "        # Display metrics at the end of each epoch.\n",
    "        train_loss = train_loss_mean.result()\n",
    "        train_acc = train_acc_metric.result()\n",
    "        t_loss.append(train_loss)\n",
    "        t_acc.append(train_acc)\n",
    "        print(\"Training Loss: %.4f Training accuracy: %.4f\" % (float(train_loss),float(train_acc),))\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        train_loss_mean.reset_states()\n",
    "        train_acc_metric.reset_states()\n",
    "        \n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        for batch in v_data:\n",
    "            test_step(batch,epoch)\n",
    "\n",
    "        val_acc = val_acc_metric.result()\n",
    "        val_loss = val_loss_mean.result()\n",
    "        \n",
    "        v_loss.append(val_loss)\n",
    "        v_acc.append(val_acc)\n",
    "        \n",
    "        print(\"Validation Loss: %.4f Validation acc: %.4f\" % (float(val_loss),float(val_acc),))\n",
    "        \n",
    "        val_loss_mean.reset_states()\n",
    "        val_acc_metric.reset_states()\n",
    "        \n",
    "        \n",
    "        # Save checkpoints\n",
    "        '''\n",
    "        if epoch % 10 == 0: \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        '''\n",
    "    \n",
    "    return epoch_l, t_loss, t_acc, v_loss, v_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c774663",
   "metadata": {},
   "source": [
    "## 4.5 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03108d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100cd72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epoch_l, t_loss, t_acc, v_loss, v_acc = train(train_data,test_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff0cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_loss1 = []\n",
    "t_acc1 = []\n",
    "v_loss1 = []\n",
    "v_acc1 = []\n",
    "\n",
    "for i in range(0,len(epoch_l)):\n",
    "    t_loss1.append(t_loss[i].numpy())\n",
    "    t_acc1.append(t_acc[i].numpy())\n",
    "    v_loss1.append(v_loss[i].numpy())\n",
    "    v_acc1.append(v_acc[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bad071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the training and validation loss and accuracies \n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "d = [epoch_l, t_loss1, v_loss1, t_acc1, v_acc1]\n",
    "export_data = zip_longest(*d, fillvalue = '')\n",
    "with open('cross_validation_results/siamese_training_vggface_cv_1.csv', 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow((\"epoch\", \"t_loss\", \"v_loss\", \"t_acc\", \"v_acc\"))\n",
    "    wr.writerows(export_data)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17685798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot the training and validation loss \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('cross_validation_results/siamese_training_vggface_cv_1.csv')\n",
    "df[['epoch', 't_loss', 'v_loss']].plot(\n",
    "    x='epoch',\n",
    "    xlabel='Epochs',\n",
    "    ylabel='Loss',\n",
    "    title='Training Loss VS Validation_Loss'\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e261359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the training and validation accuracy \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('cross_validation_results/siamese_training_vggface_cv_1.csv')\n",
    "df[['epoch', 't_acc', 'v_acc']].plot(\n",
    "    x='epoch',\n",
    "    xlabel='Epochs',\n",
    "    ylabel='Binary Accuracy',\n",
    "    title='Training Accuracy VS Validation Accuracy'\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving trained weights\n",
    "save_name = 'cross_validation_trained_models/siamesemodel_l1Dist_vgg_test_no_additional_layers.h5'\n",
    "siamese_model.save_weights(save_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
