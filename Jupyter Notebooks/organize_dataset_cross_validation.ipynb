{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69caec32",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3653419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f17ed03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies for face detection and MTCNN Model\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numpy import asarray\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b01b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard dependencies\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d04c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow dependencies - Functional API\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b502d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow dependencies - Functional API\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten, ZeroPadding2D, Convolution2D, Dropout, Activation\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9602ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Preprocessing packages\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff167b3",
   "metadata": {},
   "source": [
    "## Make a dataset for bacthes and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e290f",
   "metadata": {},
   "source": [
    "#### Description: This section is to create a good split of data in the labelled faces in the wild dataset. We make sure that there is a max of CHUNK_SIZE images per person in a batch size of BATCH_SIZE. Mainly used to attempt and make a fair split of the data so the models can train and test without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a8181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure all the names appended can be cropped\n",
    "label_idx = 0\n",
    "labels = {}\n",
    "for directory in os.listdir('lfw_cropped_faces'):\n",
    "    files = []\n",
    "    #Add all the file names of a person in a list\n",
    "    for file in os.listdir(os.path.join('lfw_cropped_faces',directory)):\n",
    "        file_name = os.path.join('lfw_cropped_faces',directory,file)\n",
    "        files.append(file_name)\n",
    "\n",
    "    labels[label_idx] = files.copy()\n",
    "    files.clear()\n",
    "    label_idx = label_idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca255682",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "files = []\n",
    "over_thirty = {}\n",
    "CHUNK_SIZE = 30\n",
    "\n",
    "#Going through all the labels\n",
    "for key, values in labels.items():\n",
    "    \n",
    "    #Adding the files of the people who have less than CHUNK_SIZE images\n",
    "    if len(values) < CHUNK_SIZE: \n",
    "        files = files + values\n",
    "        all_labels = all_labels + [key]*(len(values))\n",
    "    else:\n",
    "        over_thirty[key] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13835970",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_labels = all_labels.copy()\n",
    "og_files = files.copy()\n",
    "\n",
    "it = 0\n",
    "while True:\n",
    "    files.clear()\n",
    "    files = og_files.copy()\n",
    "    all_labels.clear()\n",
    "    all_labels = og_labels.copy()\n",
    "    \n",
    "    \n",
    "    #Going through all the photos that have over CHUNK_SIZE images\n",
    "    for key, values in over_thirty.items():\n",
    "        #Caclulating how many splits of the BATCH_SIZE fit into all the list of files\n",
    "        files_size = len(files)\n",
    "        intervals = math.ceil(files_size/BATCH_SIZE)\n",
    "\n",
    "        #Picking random order to place chuncks of files in\n",
    "        pic_idx = list(range(0,intervals))\n",
    "        random.shuffle(pic_idx)\n",
    "        idx = 0\n",
    "        \n",
    "        \n",
    "        p_img_count = len(values)\n",
    "        \n",
    "        #Split all the images into multiple chunks of CHUNK_SIZE\n",
    "        split_count =  math.ceil(p_img_count/CHUNK_SIZE)\n",
    "        c_values = [values[i:i + CHUNK_SIZE] for i in range(0, len(values), CHUNK_SIZE)]\n",
    "        \n",
    "        #Inserting the values one by one\n",
    "        for val in c_values:\n",
    "            for v in val:\n",
    "                files.insert(pic_idx[idx]*BATCH_SIZE,v)\n",
    "                all_labels.insert(pic_idx[idx]*BATCH_SIZE,key)\n",
    "            idx = idx + 1\n",
    "            if idx >= len(pic_idx):\n",
    "                idx = 0\n",
    "        \n",
    "        idx = idx + 1\n",
    "        if idx >= len(pic_idx):\n",
    "            idx = 0\n",
    "    \n",
    "    if check_good_dataset(all_labels):\n",
    "        print('Good data set!')\n",
    "        print(it)\n",
    "        break\n",
    "    it =  it + 1\n",
    "print(len(files))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e6c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to check if the dataset is done well\n",
    "def check_good_dataset(all_labels):\n",
    "    grouped_by_batch_size = [all_labels[i:i + BATCH_SIZE] for i in range(0, len(all_labels), BATCH_SIZE)]\n",
    "    i = 0\n",
    "    for g in grouped_by_batch_size:\n",
    "        contains_duplicates = [g.count(element) for element in g]\n",
    "        print(sum(contains_duplicates))\n",
    "        if sum(contains_duplicates) < 10:\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870cb792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write image file paths and labels into a csv file\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "d = [files, all_labels]\n",
    "export_data = zip_longest(*d, fillvalue = '')\n",
    "with open('fair_data_spare.csv', 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow(('files','labels'))\n",
    "    wr.writerows(export_data)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf643982",
   "metadata": {},
   "source": [
    "## Preparing Cross Validation data - Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfe3279",
   "metadata": {},
   "source": [
    "#### Description: LFW data preparation for all 5-cross validation iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "105b9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload the data set\n",
    "df = pd.read_csv('fair_data.csv', )\n",
    "\n",
    "prepared_files = df.files.to_list()\n",
    "prepared_labels = df.labels.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f49705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating data for Cross Validation iteration 1\n",
    "tr_v_files = prepared_files[:int(len(prepared_files)*0.8)]\n",
    "test_files = prepared_files[int(len(prepared_files)*0.8):]\n",
    "tr_v_labels = prepared_labels[:int(len(prepared_files)*0.8)]\n",
    "test_lables = prepared_labels[int(len(prepared_files)*0.8):]\n",
    "\n",
    "d = [tr_v_files, test_files,tr_v_labels,test_lables]\n",
    "export_data = zip_longest(*d, fillvalue = '')\n",
    "with open('cross_validation_data/data_cv_1.csv', 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow(('tr_v_files','test_files','tr_v_labels','test_lables'))\n",
    "    wr.writerows(export_data)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54293a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating data for Cross Validation iteration 2\n",
    "tr_v_files = prepared_files[:int(len(prepared_files)*0.8)]\n",
    "test_files = prepared_files[int(len(prepared_files)*0.8):]\n",
    "tr_v_labels = prepared_labels[:int(len(prepared_files)*0.8)]\n",
    "test_lables = prepared_labels[int(len(prepared_files)*0.8):]\n",
    "\n",
    "d = [tr_v_files, test_files,tr_v_labels,test_lables]\n",
    "export_data = zip_longest(*d, fillvalue = '')\n",
    "with open('cross_validation_data/data_cv_1.csv', 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow(('tr_v_files','test_files','tr_v_labels','test_lables'))\n",
    "    wr.writerows(export_data)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4150eba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2647  2648  2649 ... 13230 13231 13232] [   0    1    2 ... 2644 2645 2646]\n",
      "[    0     1     2 ... 13230 13231 13232] [2647 2648 2649 ... 5291 5292 5293]\n",
      "[    0     1     2 ... 13230 13231 13232] [5294 5295 5296 ... 7938 7939 7940]\n",
      "[    0     1     2 ... 13230 13231 13232] [ 7941  7942  7943 ... 10584 10585 10586]\n",
      "[    0     1     2 ... 10584 10585 10586] [10587 10588 10589 ... 13230 13231 13232]\n"
     ]
    }
   ],
   "source": [
    "#Splitting and saving the data into files\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "train = prepared_files\n",
    "labels = prepared_labels\n",
    "\n",
    "kf=KFold(n_splits=5)\n",
    "train = np.array(train)\n",
    "\n",
    "count = 1\n",
    "for train_index, test_index in kf.split(train):\n",
    "    print(train_index, test_index)\n",
    "    tr_v_files = []\n",
    "    test_files = []\n",
    "    tr_v_labels = []\n",
    "    test_lables = []\n",
    "    \n",
    "    for t in train_index:\n",
    "        tr_v_files.append(prepared_files[t])\n",
    "        tr_v_labels.append(prepared_labels[t])\n",
    "    \n",
    "    for tw in test_index:\n",
    "        test_files.append(prepared_files[tw])\n",
    "        test_lables.append(prepared_labels[tw])\n",
    "    \n",
    "    file_name = 'cross_validation_data/data_cv_'+str(count)+'.csv'\n",
    "    \n",
    "    d = [tr_v_files, test_files,tr_v_labels,test_lables]\n",
    "    export_data = zip_longest(*d, fillvalue = '')\n",
    "    with open(file_name, 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow(('tr_v_files','test_files','tr_v_labels','test_lables'))\n",
    "        wr.writerows(export_data)\n",
    "    myfile.close()\n",
    "    \n",
    "    count = count+1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ca74a",
   "metadata": {},
   "source": [
    "## Peparing binary inputs for Siamese Network for each cross validation training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e05785",
   "metadata": {},
   "source": [
    "#### Description: Preparing data into two input pairs because our siamese networks will take two input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5c55030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num in range(1,6):\n",
    "    #Preparation for each iteration\n",
    "    \n",
    "    #Load images\n",
    "    file_name = 'cross_validation_data/data_cv_'+str(num)+'.csv'\n",
    "    df = pd.read_csv(file_name, )\n",
    "    tr_files = df.tr_v_files.to_list()\n",
    "    tr_labels = df.tr_v_labels.to_list()\n",
    "    \n",
    "    #Empty arrays\n",
    "    anchor = []\n",
    "    other_image = []\n",
    "    binary = []\n",
    "    anchor_label = []\n",
    "    other_image_label = []\n",
    "    \n",
    "    # Making a positive and negative for each image (repetitions will probably happen)\n",
    "    for i in range(0,len(tr_labels)):\n",
    "        temp = tr_labels[i]\n",
    "        all_idx = [index for index, element in enumerate(tr_labels) if element == temp]\n",
    "        if len(all_idx) > 1:\n",
    "            #Shuffling index all potential positives\n",
    "            ran = random.randint(0,len(all_idx)-1)\n",
    "\n",
    "            while all_idx[ran] == i:\n",
    "                ran = random.randint(0,len(all_idx)-1)\n",
    "\n",
    "            #Adding positive match\n",
    "            anchor.append(tr_files[i])\n",
    "            other_image.append(tr_files[all_idx[ran]])\n",
    "            binary.append(1)\n",
    "            anchor_label.append(tr_labels[i])\n",
    "            other_image_label.append(tr_labels[all_idx[ran]])\n",
    "\n",
    "\n",
    "            #Shuffling index all potential positives\n",
    "            ran_n = random.randint(0,len(tr_labels)-1)\n",
    "\n",
    "            while ran_n == i or ran_n in all_idx:\n",
    "                ran_n = random.randint(0,len(tr_labels)-1)\n",
    "\n",
    "            #Adding negative match\n",
    "            anchor.append(tr_files[i])\n",
    "            other_image.append(tr_files[ran_n])\n",
    "            binary.append(0)\n",
    "            anchor_label.append(tr_labels[i])\n",
    "            other_image_label.append(tr_labels[ran_n])\n",
    "            \n",
    "    d = [anchor, other_image, binary,anchor_label,other_image_label]\n",
    "    export_data = zip_longest(*d, fillvalue = '')\n",
    "    \n",
    "    \n",
    "    file_name_save = 'cross_validation_data/siamese_training_data_cv_'+str(num)+'.csv'\n",
    "    with open(file_name_save, 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow(('anchor','other_image','binary','anchor_label','other_image_label'))\n",
    "        wr.writerows(export_data)\n",
    "    myfile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07404b",
   "metadata": {},
   "source": [
    "## Peparing binary inputs for Siamese Network for each cross validation test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ad3b2",
   "metadata": {},
   "source": [
    "#### Description: Preparing data into two input pairs because our siamese networks will take two input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76ded608",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(1,6):\n",
    "    #Preparation for each iteration\n",
    "    \n",
    "    #Load images\n",
    "    file_name = 'cross_validation_data/data_cv_'+str(num)+'.csv'\n",
    "    df = pd.read_csv(file_name, )\n",
    "    tr_files = df.test_files.to_list()\n",
    "    tr_labels = df.test_lables.to_list()\n",
    "    \n",
    "    #Removng nan values\n",
    "    tr_files = [x for x in tr_files if str(x) != 'nan']\n",
    "    tr_labels = [x for x in tr_labels if math.isnan(x) == False]\n",
    "    \n",
    "    #Empty arrays\n",
    "    anchor = []\n",
    "    other_image = []\n",
    "    binary = []\n",
    "    anchor_label = []\n",
    "    other_image_label = []\n",
    "    \n",
    "    # Making a positive and negative for each image (repetitions will probably happen)\n",
    "    for i in range(0,len(tr_labels)):\n",
    "        temp = tr_labels[i]\n",
    "        all_idx = [index for index, element in enumerate(tr_labels) if element == temp]\n",
    "        if len(all_idx) > 1:\n",
    "            #Shuffling index all potential positives\n",
    "            ran = random.randint(0,len(all_idx)-1)\n",
    "\n",
    "            while all_idx[ran] == i:\n",
    "                ran = random.randint(0,len(all_idx)-1)\n",
    "\n",
    "            #Adding positive match\n",
    "            anchor.append(tr_files[i])\n",
    "            other_image.append(tr_files[all_idx[ran]])\n",
    "            binary.append(1)\n",
    "            anchor_label.append(tr_labels[i])\n",
    "            other_image_label.append(tr_labels[all_idx[ran]])\n",
    "\n",
    "\n",
    "            #Shuffling index all potential positives\n",
    "            ran_n = random.randint(0,len(tr_labels)-1)\n",
    "\n",
    "            while ran_n == i or ran_n in all_idx:\n",
    "                ran_n = random.randint(0,len(tr_labels)-1)\n",
    "\n",
    "            #Adding negative match\n",
    "            anchor.append(tr_files[i])\n",
    "            other_image.append(tr_files[ran_n])\n",
    "            binary.append(0)\n",
    "            anchor_label.append(tr_labels[i])\n",
    "            other_image_label.append(tr_labels[ran_n])\n",
    "            \n",
    "    d = [anchor, other_image, binary,anchor_label,other_image_label]\n",
    "    export_data = zip_longest(*d, fillvalue = '')\n",
    "    \n",
    "    \n",
    "    file_name_save = 'cross_validation_data/siamese_testing_data_cv_'+str(num)+'.csv'\n",
    "    with open(file_name_save, 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow(('anchor','other_image','binary','anchor_label','other_image_label'))\n",
    "        wr.writerows(export_data)\n",
    "    myfile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c077ceb4",
   "metadata": {},
   "source": [
    "## Peparing binary inputs for demo data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d6ed4",
   "metadata": {},
   "source": [
    "#### Description: Preparing data into two input pairs because our siamese networks will take two input images. This is specifically for demonstration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "605902c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "count = 0\n",
    "for directory in os.listdir('all_demo_data_cropped'):\n",
    "    label_dict[directory] = count\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81634992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty arrays\n",
    "anchor = []\n",
    "other_image = []\n",
    "binary = []\n",
    "anchor_label = []\n",
    "other_image_label = []\n",
    "count = 0\n",
    "\n",
    "#Making dataset for ID and all other angles other than straight faces\n",
    "for directory in os.listdir('all_demo_data_cropped'):\n",
    "    \n",
    "    for directory2 in os.listdir('all_demo_data_cropped'):\n",
    "        same = 0\n",
    "        if directory == directory2:\n",
    "            same = 1\n",
    "\n",
    "        anchor.append(os.path.join('all_demo_data_cropped',directory,'ID.jpg'))\n",
    "        other_image.append(os.path.join('all_demo_data_cropped',directory2,'top.jpg'))\n",
    "        binary.append(same)\n",
    "        anchor_label.append(label_dict[directory])\n",
    "        other_image_label.append(label_dict[directory2])\n",
    "\n",
    "        anchor.append(os.path.join('all_demo_data_cropped',directory,'ID.jpg'))\n",
    "        other_image.append(os.path.join('all_demo_data_cropped',directory2,'bottom.jpg'))\n",
    "        binary.append(same)\n",
    "        anchor_label.append(label_dict[directory])\n",
    "        other_image_label.append(label_dict[directory2])\n",
    "\n",
    "        anchor.append(os.path.join('all_demo_data_cropped',directory,'ID.jpg'))\n",
    "        other_image.append(os.path.join('all_demo_data_cropped',directory2,'left.jpg'))\n",
    "        binary.append(same)\n",
    "        anchor_label.append(label_dict[directory])\n",
    "        other_image_label.append(label_dict[directory2])\n",
    "        \n",
    "        anchor.append(os.path.join('all_demo_data_cropped',directory,'ID.jpg'))\n",
    "        other_image.append(os.path.join('all_demo_data_cropped',directory2,'right.jpg'))\n",
    "        binary.append(same)\n",
    "        anchor_label.append(label_dict[directory])\n",
    "        other_image_label.append(label_dict[directory2])\n",
    "\n",
    "d = [anchor, other_image, binary,anchor_label,other_image_label]\n",
    "export_data = zip_longest(*d, fillvalue = '')        \n",
    "\n",
    "file_name_save = 'cross_validation_data/siamese_demonstration_not_straight_faces_data.csv'\n",
    "with open(file_name_save, 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow(('anchor','other_image','binary','anchor_label','other_image_label'))\n",
    "    wr.writerows(export_data)\n",
    "myfile.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "898a6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty arrays\n",
    "anchor = []\n",
    "other_image = []\n",
    "binary = []\n",
    "anchor_label = []\n",
    "other_image_label = []\n",
    "count = 0\n",
    "\n",
    "#Making dataset for ID and straight faces\n",
    "for directory in os.listdir('all_demo_data_cropped'):\n",
    "    \n",
    "    for directory2 in os.listdir('all_demo_data_cropped'):\n",
    "        same = 0\n",
    "        if directory == directory2:\n",
    "            same = 1\n",
    "\n",
    "        anchor.append(os.path.join('all_demo_data_cropped',directory,'ID.jpg'))\n",
    "        other_image.append(os.path.join('all_demo_data_cropped',directory2,'straight.jpg'))\n",
    "        binary.append(same)\n",
    "        anchor_label.append(label_dict[directory])\n",
    "        other_image_label.append(label_dict[directory2])\n",
    "\n",
    "\n",
    "d = [anchor, other_image, binary,anchor_label,other_image_label]\n",
    "export_data = zip_longest(*d, fillvalue = '')        \n",
    "\n",
    "file_name_save = 'cross_validation_data/siamese_demonstration_all_straight_faces_data.csv'\n",
    "with open(file_name_save, 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow(('anchor','other_image','binary','anchor_label','other_image_label'))\n",
    "    wr.writerows(export_data)\n",
    "myfile.close()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "246dcd69f4a8361d6805b1d71b04e68adc1e037a7b9126fe780becdac55e1c8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
