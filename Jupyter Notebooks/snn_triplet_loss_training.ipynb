{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d42c91da",
   "metadata": {
    "id": "d42c91da"
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ad990",
   "metadata": {
    "id": "e40ad990"
   },
   "source": [
    "## 1.1 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60acf98d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "60acf98d",
    "outputId": "f8430aa7-a21d-4d86-bce8-caca811321c5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.5 tensorflow-gpu==2.5 opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "005972d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./jupyterenv/lib/python3.6/site-packages (2.6.2)\n",
      "Requirement already satisfied: absl-py~=0.10 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (0.15.0)\n",
      "Requirement already satisfied: six~=1.15.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: gast==0.4.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: tensorboard<2.7,>=2.6.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: keras<2.7,>=2.6.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (1.46.3)\n",
      "Requirement already satisfied: clang~=5.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel~=0.35 in ./jupyterenv/lib/python3.6/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: cached-property in ./jupyterenv/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./jupyterenv/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in ./jupyterenv/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./jupyterenv/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (59.6.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./jupyterenv/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./jupyterenv/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./jupyterenv/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./jupyterenv/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./jupyterenv/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./jupyterenv/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./jupyterenv/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (4.8.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./jupyterenv/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./jupyterenv/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./jupyterenv/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./jupyterenv/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: dataclasses in ./jupyterenv/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow) (0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in ./jupyterenv/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./jupyterenv/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./jupyterenv/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e7286",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "760e7286",
    "outputId": "32e11469-92a9-4b0a-ddb1-6fe078e7dfde",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install matplotlib\n",
    "!pip3 install keras\n",
    "!pip3 install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb0121",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05eb0121",
    "outputId": "1a24a84e-f902-4d85-d8d6-98386c37d130",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install keras==2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f700b1a",
   "metadata": {
    "id": "4f700b1a"
   },
   "source": [
    "## 1.2 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3954993d",
   "metadata": {
    "id": "3954993d"
   },
   "outputs": [],
   "source": [
    "# Import standard dependencies\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d16ec1",
   "metadata": {
    "id": "12d16ec1"
   },
   "outputs": [],
   "source": [
    "# Import Dependencies for face detection and MTCNN Model\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numpy import asarray\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb77b0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fb77b0a",
    "outputId": "4102e8b4-00cc-4f4c-e18a-3df1ab07622e"
   },
   "outputs": [],
   "source": [
    "#Import Preprocessing packages\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b13c69",
   "metadata": {
    "id": "56b13c69"
   },
   "outputs": [],
   "source": [
    "# VGG16 model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a0b9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logger for training\n",
    "from tensorflow.keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a23c83c4",
   "metadata": {
    "id": "a23c83c4"
   },
   "outputs": [],
   "source": [
    "# Import tensorflow dependencies - Functional API\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten, ZeroPadding2D, Convolution2D, Dropout, Activation\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1802e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b21dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of the input images\n",
    "target_shape = (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5abe0d",
   "metadata": {
    "id": "0b5abe0d"
   },
   "source": [
    "## 1.3 Set GPU Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf926b87",
   "metadata": {
    "id": "cf926b87"
   },
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8287acd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-72d594a7d345>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fWCHYrCfpuTA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWCHYrCfpuTA",
    "outputId": "460fcb74-7705-4aa9-c383-32dcbd1d3db6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd525c8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ebe13b",
   "metadata": {
    "id": "23ebe13b"
   },
   "source": [
    "# 2. Collect Positives and Anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f7a9e",
   "metadata": {
    "id": "f86f7a9e"
   },
   "source": [
    "## 2.1 Untar Labelled Faces in the Wild Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217244a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "217244a3",
    "outputId": "a70434de-3111-4e96-d024-8f4ce0d04581"
   },
   "outputs": [],
   "source": [
    "# Uncompress Tar GZ Labelled Faces in the Wild Dataset\n",
    "!tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322d0e7",
   "metadata": {
    "id": "5322d0e7"
   },
   "outputs": [],
   "source": [
    "# Move LFW Images to the following repository data/negative\n",
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw', directory)):\n",
    "        EX_PATH = os.path.join('lfw', directory, file)\n",
    "        NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "        os.replace(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b273792",
   "metadata": {
    "id": "6b273792"
   },
   "source": [
    "# 3. Load and Preprocess Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e54766f",
   "metadata": {
    "id": "7e54766f"
   },
   "source": [
    "## 3.1 Preprocessing - Resizing, Alignment and Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dddb37c1",
   "metadata": {
    "id": "ef952ecd"
   },
   "outputs": [],
   "source": [
    "# Preprocess images\n",
    "def preprocess(file_path, required_size=(224,224)):\n",
    "    raw = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(raw, channels=3, dct_method='INTEGER_ACCURATE')\n",
    "    image = tf.image.resize(image,required_size, method='nearest')\n",
    "    image = tf.cast(image, 'float32')\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cfd73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = tf.keras.backend\n",
    "\n",
    "# VGG Face preprocessing\n",
    "def preprocess_input_vvgface(x, data_format=None, version=1):\n",
    "    x_temp = np.copy(x)\n",
    "    if data_format is None:\n",
    "        data_format = K.image_data_format()\n",
    "    assert data_format in {'channels_last', 'channels_first'}\n",
    "\n",
    "    if version == 1:\n",
    "        if data_format == 'channels_first':\n",
    "            x_temp = x_temp[:, ::-1, ...]\n",
    "            x_temp[:, 0, :, :] -= 93.5940\n",
    "            x_temp[:, 1, :, :] -= 104.7624\n",
    "            x_temp[:, 2, :, :] -= 129.1863\n",
    "        else:\n",
    "            x_temp = x_temp[..., ::-1]\n",
    "            x_temp[..., 0] -= 93.5940\n",
    "            x_temp[..., 1] -= 104.7624\n",
    "            x_temp[..., 2] -= 129.1863\n",
    "\n",
    "    elif version == 2:\n",
    "        if data_format == 'channels_first':\n",
    "            x_temp = x_temp[:, ::-1, ...]\n",
    "            x_temp[:, 0, :, :] -= 91.4953\n",
    "            x_temp[:, 1, :, :] -= 103.8827\n",
    "            x_temp[:, 2, :, :] -= 131.0912\n",
    "        else:\n",
    "            x_temp = x_temp[..., ::-1]\n",
    "            x_temp[..., 0] -= 91.4953\n",
    "            x_temp[..., 1] -= 103.8827\n",
    "            x_temp[..., 2] -= 131.0912\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return x_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bce41fb8",
   "metadata": {
    "id": "bce41fb8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def complete_preprocess(image_path, required_size=(224,224)): \n",
    "    # load image and detect the face\n",
    "    image = preprocess(image_path)\n",
    "    \n",
    "    #image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    #Preprocessing\n",
    "    face_array = preprocess_input(image)\n",
    "    \n",
    "    \n",
    "    # Scale image to be between 0 and 1 \n",
    "    face_array = (face_array - np.amin(face_array)) / (np.amax(face_array) - np.amin(face_array))\n",
    "    # Scale image to be between -1 and 1\n",
    "    face_array = 2*face_array - 1\n",
    "    \n",
    "    return tf.convert_to_tensor(face_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907006fe",
   "metadata": {
    "id": "907006fe"
   },
   "source": [
    "# 3.2 Create Labelled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ea837152",
   "metadata": {
    "id": "ea837152"
   },
   "outputs": [],
   "source": [
    "#Peparing arrays of file paths of all the data\n",
    "anchor_array = []\n",
    "positive_array = []\n",
    "negative_array = []\n",
    "\n",
    "#Making sure all the names appended can be cropped\n",
    "for directory in os.listdir('cropped_test_data'):\n",
    "    anchor_name = os.path.join('cropped_test_data',directory,'anchor.jpg')\n",
    "    positive_name = os.path.join('cropped_test_data',directory,'positive.jpg')\n",
    "    negative_name = os.path.join('cropped_test_data',directory,'negative.jpg')\n",
    "    \n",
    "    \n",
    "    anchor_array.append(anchor_name)\n",
    "    positive_array.append(positive_name)\n",
    "    negative_array.append(negative_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "61605f5a",
   "metadata": {
    "id": "61605f5a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(anchor_array), \n",
    "                            tf.data.Dataset.from_tensor_slices(positive_array), \n",
    "                            tf.data.Dataset.from_tensor_slices(negative_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure all the names appended can be cropped\n",
    "label_idx = 0\n",
    "labels = []\n",
    "files = []\n",
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw',directory)):\n",
    "        file_name = os.path.join('lfw',directory,file)\n",
    "        files.append(file_name)\n",
    "        labels.append(label_idx)\n",
    "        print(file_name)\n",
    "        print(label_idx)\n",
    "    \n",
    "    label_idx = label_idx + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0f9637b1",
   "metadata": {
    "id": "61605f5a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(files), \n",
    "                            tf.data.Dataset.from_tensor_slices(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d55a3",
   "metadata": {
    "id": "488d55a3"
   },
   "source": [
    "# 3.4 Build Train and Test Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c8602978",
   "metadata": {
    "id": "c8602978"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(anchor_img, positive_img, negative_img):\n",
    "    return(complete_preprocess(anchor_img), complete_preprocess(positive_img), complete_preprocess(negative_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b7ba47e8",
   "metadata": {
    "id": "c8602978"
   },
   "outputs": [],
   "source": [
    "def preprocess_twin(anchor_img, label):\n",
    "    return(complete_preprocess(anchor_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "01dc1a5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# * is to unpack the values inside current_sample\n",
    "res = preprocess_twin(*current_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ba4a1b7c",
   "metadata": {
    "id": "ba4a1b7c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build dataloader pipeline\n",
    "data = data.map(lambda x, y: tf.py_function(preprocess_twin, inp = (x, y), Tout=(tf.float32, tf.int32)))\n",
    "#data = data.map(preprocess_twin)\n",
    "#data = data.cache()\n",
    "#data = data.shuffle(buffer_size=1024, seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "33ec504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloader pipeline for the preprocess_dataset function\n",
    "#data = data.map(preprocess_twin)\n",
    "data = data.map(lambda x, y , z: tf.py_function(preprocess_dataset, inp = (x, y, z), Tout=(tf.float32, tf.float32, tf.float32)))\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024, seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8764693e",
   "metadata": {
    "id": "8764693e"
   },
   "outputs": [],
   "source": [
    "# Training partition\n",
    "train_data = data.take(round(len(data)*0.7)) #Getting 100% of images\n",
    "train_data = train_data.batch(120)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4e98e2a0",
   "metadata": {
    "id": "4e98e2a0"
   },
   "outputs": [],
   "source": [
    "# Testing partition\n",
    "test_data = data.skip(round(len(data)*.7))\n",
    "test_data = test_data.take(round(len(data)*.3))\n",
    "test_data = test_data.batch(120)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce802c6d",
   "metadata": {
    "id": "ce802c6d"
   },
   "source": [
    "# 4. Model Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2adb81",
   "metadata": {
    "id": "bb2adb81"
   },
   "source": [
    "## 4.1 Load VGG_Face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bc57c03",
   "metadata": {
    "id": "4bc57c03"
   },
   "outputs": [],
   "source": [
    "vgg_model = keras.Sequential()\n",
    "vgg_model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "vgg_model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "vgg_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "vgg_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "vgg_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "vgg_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "vgg_model.add(ZeroPadding2D((1,1)))\n",
    "vgg_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "vgg_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vgg_model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "vgg_model.add(Dropout(0.5))\n",
    "vgg_model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "vgg_model.add(Dropout(0.5))\n",
    "vgg_model.add(Convolution2D(2622, (1, 1)))\n",
    "vgg_model.add(Flatten())\n",
    "vgg_model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98fd43db",
   "metadata": {
    "id": "98fd43db"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "vgg_model.load_weights('vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5671a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To pop the bottom activation layer and adding another dense node\n",
    "#vgg_model.pop()\n",
    "#vgg_model.add(Dense(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de16178",
   "metadata": {
    "id": "6de16178",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg_face_embedding = Model(inputs=vgg_model.layers[0].input, outputs=vgg_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189429a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "189429a6",
    "outputId": "7138bac9-9a8d-409e-cc2d-4513d795ad06",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_input (InputL [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 226, 226, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 226, 226, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 114, 114, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 114, 114, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 58, 58, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 1, 1, 4096)        102764544 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 1, 1, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 1, 1, 2622)        10742334  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2622)              0         \n",
      "=================================================================\n",
      "Total params: 145,002,878\n",
      "Trainable params: 145,002,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_face_embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11ec254",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e11ec254",
    "outputId": "071f126a-e468-4719-ecbc-476c42389c45",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 zero_padding2d_52_input False\n",
      "1 zero_padding2d_52 False\n",
      "2 conv2d_64 False\n",
      "3 zero_padding2d_53 False\n",
      "4 conv2d_65 False\n",
      "5 max_pooling2d_20 False\n",
      "6 zero_padding2d_54 False\n",
      "7 conv2d_66 False\n",
      "8 zero_padding2d_55 False\n",
      "9 conv2d_67 False\n",
      "10 max_pooling2d_21 False\n",
      "11 zero_padding2d_56 False\n",
      "12 conv2d_68 False\n",
      "13 zero_padding2d_57 False\n",
      "14 conv2d_69 False\n",
      "15 zero_padding2d_58 False\n",
      "16 conv2d_70 False\n",
      "17 max_pooling2d_22 False\n",
      "18 zero_padding2d_59 False\n",
      "19 conv2d_71 False\n",
      "20 zero_padding2d_60 False\n",
      "21 conv2d_72 False\n",
      "22 zero_padding2d_61 False\n",
      "23 conv2d_73 False\n",
      "24 max_pooling2d_23 False\n",
      "25 zero_padding2d_62 False\n",
      "26 conv2d_74 False\n",
      "27 zero_padding2d_63 False\n",
      "28 conv2d_75 False\n",
      "29 zero_padding2d_64 False\n",
      "30 conv2d_76 False\n",
      "31 max_pooling2d_24 False\n",
      "32 flatten_5 True\n",
      "33 dense_3 True\n",
      "34 dropout_12 True\n",
      "35 dense_4 True\n",
      "36 dropout_13 True\n",
      "37 dense_5 True\n"
     ]
    }
   ],
   "source": [
    "# Freeze four convolution blocks\n",
    "for layer in vgg_face_embedding.layers[:32]:\n",
    "    layer.trainable = False\n",
    "# Make sure you have frozen the correct layers\n",
    "for i, layer in enumerate(vgg_face_embedding.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889350c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "889350c0",
    "outputId": "97108412-0134-4064-dd13-3926da6ef711",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_52_input (Inp [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_52 (ZeroPaddi (None, 226, 226, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_64 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_53 (ZeroPaddi (None, 226, 226, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_54 (ZeroPaddi (None, 114, 114, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_55 (ZeroPaddi (None, 114, 114, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_56 (ZeroPaddi (None, 58, 58, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_57 (ZeroPaddi (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_58 (ZeroPaddi (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_59 (ZeroPaddi (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_60 (ZeroPaddi (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_61 (ZeroPaddi (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_62 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_63 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_64 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2622)              10742334  \n",
      "=================================================================\n",
      "Total params: 145,002,878\n",
      "Trainable params: 130,288,190\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_face_embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f7e148",
   "metadata": {
    "id": "57f7e148"
   },
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065256d",
   "metadata": {
    "id": "0065256d"
   },
   "source": [
    "## 5.1 Setup Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(0.01) # 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6982784",
   "metadata": {
    "id": "b6982784"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(0.01) # 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe6f77",
   "metadata": {
    "id": "f9fe6f77"
   },
   "outputs": [],
   "source": [
    "#Loss function\n",
    "def triplet_loss(anchor,positive,negative, alpha=0.3):\n",
    "    positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis=-1)\n",
    "    negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis=-1)\n",
    "    return tf.maximum(positive_dist - negative_dist + alpha, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceLayer(Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0438846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the metrics\n",
    "train_acc_metric = keras.metrics.BinaryAccuracy()\n",
    "val_acc_metric = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "train_loss_mean = tf.keras.metrics.Mean()\n",
    "val_loss_mean = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41176b",
   "metadata": {
    "id": "df41176b"
   },
   "source": [
    "## 5.2 Establish Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463356dd",
   "metadata": {
    "id": "463356dd"
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = os.path.join('training_checkpoints')\n",
    "os.makedirs(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df39da",
   "metadata": {
    "id": "f5df39da"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdddf21",
   "metadata": {
    "id": "5cdddf21"
   },
   "source": [
    "## 5.3 Build Train Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d855c",
   "metadata": {
    "id": "739d855c"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    # Record all of our operations \n",
    "    with tf.GradientTape() as tape:     \n",
    "        \n",
    "        # Forward pass\n",
    "        anchor = vgg_face_embedding(batch[0], training=True)\n",
    "        positive = vgg_face_embedding(batch[1], training=True)\n",
    "        negative = vgg_face_embedding(batch[2], training=True)\n",
    "        # Calculate loss\n",
    "        loss = triplet_loss(anchor,positive,negative)\n",
    "        \n",
    "    # Calculate gradients\n",
    "    grad = tape.gradient(loss, vgg_face_embedding.trainable_variables)\n",
    "    \n",
    "    # Calculate updated weights and apply to siamese model\n",
    "    opt.apply_gradients(zip(grad, vgg_face_embedding.trainable_variables))\n",
    "    \n",
    "    #Update loss metric\n",
    "    train_loss_mean.update_state(loss)\n",
    "    \n",
    "    # Return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(batch):\n",
    "    \n",
    "   # Forward pass\n",
    "    anchor = vgg_face_embedding(batch[0], training=False)\n",
    "    positive = vgg_face_embedding(batch[1], training=False)\n",
    "    negative = vgg_face_embedding(batch[2], training=False)\n",
    "    # Calculate loss\n",
    "    loss = triplet_loss(anchor,positive,negative)\n",
    "\n",
    "    val_loss_mean.update_state(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff7280e",
   "metadata": {
    "id": "3ff7280e"
   },
   "source": [
    "## 5.4 Build Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63006370",
   "metadata": {
    "id": "63006370"
   },
   "outputs": [],
   "source": [
    "def train(data, v_data, EPOCHS):\n",
    "    epoch_l = []\n",
    "    t_loss = []\n",
    "    t_acc = []\n",
    "    v_loss = []\n",
    "    v_acc = []\n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        epoch_l.append(epoch)\n",
    "        \n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step here\n",
    "            loss_value = train_step(batch)\n",
    "            progbar.update(idx+1)\n",
    "        \n",
    "        # Display metrics at the end of each epoch.\n",
    "        train_loss = train_loss_mean.result()\n",
    "        train_acc = train_acc_metric.result()\n",
    "        t_loss.append(train_loss)\n",
    "        t_acc.append(train_acc)\n",
    "        print(\"Training Loss: %.4f Training accuracy: %.4f\" % (float(train_loss),float(train_acc),))\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        train_loss_mean.reset_states()\n",
    "        train_acc_metric.reset_states()\n",
    "        \n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        for batch in v_data:\n",
    "            test_step(batch)\n",
    "\n",
    "        val_acc = val_acc_metric.result()\n",
    "        val_loss = val_loss_mean.result()\n",
    "        \n",
    "        v_loss.append(val_loss)\n",
    "        v_acc.append(val_acc)\n",
    "        \n",
    "        print(\"Validation Loss: %.4f Validation acc: %.4f\" % (float(val_loss),float(val_acc),))\n",
    "        \n",
    "        val_loss_mean.reset_states()\n",
    "        val_acc_metric.reset_states()\n",
    "        \n",
    "        \n",
    "        # Save checkpoints\n",
    "        '''\n",
    "        if epoch % 10 == 0: \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        '''\n",
    "    \n",
    "    return epoch_l, t_loss, t_acc, v_loss, v_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f673e",
   "metadata": {
    "id": "618f673e"
   },
   "source": [
    "## 5.5 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e49d3",
   "metadata": {
    "id": "5f0e49d3"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f1a11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b7f1a11",
    "outputId": "3445b69c-261f-41c2-ff99-854692e49182",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_l, t_loss, t_acc, v_loss, v_acc = train(train_data,test_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0845d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_loss1 = []\n",
    "t_acc1 = []\n",
    "v_loss1 = []\n",
    "v_acc1 = []\n",
    "\n",
    "for i in range(0,len(epoch_l)):\n",
    "    t_loss1.append(t_loss[i].numpy())\n",
    "    t_acc1.append(t_acc[i].numpy())\n",
    "    v_loss1.append(v_loss[i].numpy())\n",
    "    v_acc1.append(v_acc[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d414fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving all the training and validation loss and accuracies from training are saved into a csv\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "d = [epoch_l, t_loss1, v_loss1, t_acc1, v_acc1]\n",
    "export_data = zip_longest(*d, fillvalue = '')\n",
    "with open('log_vgg_triplet_training_6_6.csv', 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "      wr = csv.writer(myfile)\n",
    "      wr.writerow((\"epoch\", \"t_loss\", \"v_loss\", \"t_acc\", \"v_acc\"))\n",
    "      wr.writerows(export_data)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461b9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Open the graph\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('log_vgg_triplet_training_6_6.csv')\n",
    "df[['epoch', 't_loss', 'v_loss']].plot(\n",
    "    x='epoch',\n",
    "    xlabel='x',\n",
    "    ylabel='y',\n",
    "    title='Loss VS Validation_Loss'\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836eb3d",
   "metadata": {},
   "source": [
    "# 5.6 Training another way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e478fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = (224,224)\n",
    "anchor_input = Input(name=\"anchor\", shape=target_shape + (3,))\n",
    "positive_input = Input(name=\"positive\", shape=target_shape + (3,))\n",
    "negative_input = Input(name=\"negative\", shape=target_shape + (3,))\n",
    "\n",
    "'''\n",
    "#L2 normalize embeddings\n",
    "distances = DistanceLayer()(\n",
    "    tf.math.l2_normalize(vgg_face_embedding(anchor_input), axis=-1),\n",
    "    tf.math.l2_normalize(vgg_face_embedding(positive_input), axis=-1),\n",
    "    tf.math.l2_normalize(vgg_face_embedding(negative_input), axis=-1)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "distances = DistanceLayer()(\n",
    "    vgg_face_embedding(anchor_input),\n",
    "    vgg_face_embedding(positive_input),\n",
    "    vgg_face_embedding(negative_input),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda2ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor (InputLayer)             [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_4 (Functional)            (None, 2622)         145002878   anchor[0][0]                     \n",
      "                                                                 positive[0][0]                   \n",
      "                                                                 negative[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "distance_layer_2 (DistanceLayer ((None,), (None,))   0           model_4[0][0]                    \n",
      "                                                                 model_4[1][0]                    \n",
      "                                                                 model_4[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 145,002,878\n",
      "Trainable params: 130,288,190\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.2):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e6f0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_model = SiameseModel(siamese_network)\n",
    "training_model.compile(optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db1040",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger('log_train_2622D_Softmax_3_6.csv', append=True, separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b8393",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "History = training_model.fit(train_data,batch_size=32, epochs=100, validation_data=test_data, callbacks=[csv_logger]) \n",
    "train_loss = History.history['loss']\n",
    "val_loss   = History.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0de50",
   "metadata": {
    "id": "6fc0de50"
   },
   "source": [
    "# 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08857e92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68107525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing just the CNN layer, the name \"model_5\" might be wrong, double check using the summary() of the siamese model\n",
    "new_embedding = siamese_network.get_layer('model_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e839c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd9029",
   "metadata": {
    "id": "c8cd9029",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "new_embedding.save('snn_gpu_trained_model_128D_preprocessed_1_6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8176445",
   "metadata": {
    "id": "a8176445",
    "outputId": "ee08c4f3-d8ca-4fa9-ffab-da31602d099b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L1Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f029c",
   "metadata": {
    "id": "070f029c",
    "outputId": "83b9b7e4-50de-47a4-9da9-2b239d582a6c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reload model \n",
    "siamese_model = tf.keras.models.load_model('gpu_trained_model.h5', \n",
    "                                   custom_objects={'L1Dist':L1Dist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33292d38",
   "metadata": {
    "id": "33292d38",
    "outputId": "9bdf3944-ec36-46e3-eba0-ce066345391e"
   },
   "outputs": [],
   "source": [
    "# View model summary\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4169d",
   "metadata": {
    "id": "9ed4169d"
   },
   "source": [
    "# 7. Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e69d3",
   "metadata": {
    "id": "c97e69d3",
    "outputId": "9e45eaeb-504e-41a3-f7a6-d8705efd9222",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sources:\n",
    "https://morioh.com/p/a07857cbc76d\n",
    "https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/\n",
    "https://keras.io/examples/vision/siamese_contrastive/ #For euclidean distance\n",
    "https://keras.io/examples/vision/siamese_network/#inspecting-what-the-network-has-learned\n",
    "https://github.com/keras-team/keras-io/blob/master/examples/vision/siamese_network.py\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "e051d0ef",
    "f86f7a9e",
    "bf4a2dee",
    "ec7f4b10",
    "7f8f1e1f",
    "0065256d",
    "df41176b",
    "5cdddf21",
    "3ff7280e",
    "618f673e",
    "6f467a2b",
    "64e995ac",
    "37e3f295",
    "48596394",
    "d04e79b1",
    "c21a698a",
    "0a74645d"
   ],
   "name": "vgg16_snn_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
